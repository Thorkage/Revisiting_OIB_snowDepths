{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "%matplotlib widget\n",
    "\n",
    "from msc_thesis_functions import *\n",
    "del sys.modules['msc_thesis_functions']\n",
    "from msc_thesis_functions import *\n",
    "\n",
    "from scipy.stats import linregress\n",
    "import seaborn as sns\n",
    "\n",
    "import cmocean as cmo\n",
    "import cmasher as cmr\n",
    "import pyproj\n",
    "import verde as vd\n",
    "\n",
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from haversine import haversine, Unit, inverse_haversine\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.path as mpath\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from pydlc import dense_lines\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import cartopy\n",
    "from cartopy.mpl import geoaxes\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "from cartopy.geodesic import Geodesic\n",
    "\n",
    "import shapely\n",
    "from shapely.geometry import Polygon, Point, MultiPoint, LineString\n",
    "from shapely.geometry import box\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from time import sleep\n",
    "import rioxarray\n",
    "\n",
    "#Variogram \n",
    "import gstools as gs\n",
    "# import xdem\n",
    "from xdem.terrain import get_terrain_attribute\n",
    "\n",
    "# import skgstat as skg\n",
    "# from geopy.distance import vincenty\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "#Kriging (pykrige, gstoools, skg)\n",
    "import pykrige.kriging_tools as kt\n",
    "\n",
    "from pykrige.ok import OrdinaryKriging\n",
    "del sys.modules['pykrige.ok']\n",
    "from pykrige.ok import OrdinaryKriging\n",
    "\n",
    "from pykrige.uk import UniversalKriging\n",
    "\n",
    "\n",
    "# Building a 'Blues' colormap where the lower end (white) is transperant\n",
    "ncolors = 256\n",
    "color_array = plt.get_cmap('Blues')(range(ncolors))\n",
    "color_array[:,-1] = np.linspace(0.0,1.0,ncolors)\n",
    "map_object = LinearSegmentedColormap.from_list(name='Blues_alpha',colors=color_array)\n",
    "\n",
    "# register this new colormap with matplotlib\n",
    "# plt.colormaps.register(cmap=map_object)\n",
    "\n",
    "from pyfonts import load_font\n",
    "\n",
    "# font = load_font(\n",
    "#    \"https://github.com/google/fonts/blob/main/ofl/roboto/Roboto[wdth,wght].ttf?raw=true\"\n",
    "# )\n",
    "\n",
    "from pyproj import Transformer\n",
    "transformer = Transformer.from_crs(4326, 3413, always_xy=True)\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_level_ice_elevation(elevs):\n",
    "    df = pd.DataFrame({'elevation': elevs})\n",
    "        \n",
    "    df['elev_quantile'] = pd.qcut(elevs, q=100,   labels=False, duplicates='drop')\n",
    "    min_ind = (df.groupby('elev_quantile')['elevation'].mean().diff().argmin() - 10 , df.groupby('elev_quantile')['elevation'].mean().diff().argmin() + 10)\n",
    "    level_ice_elevation = df.groupby('elev_quantile')['elevation'].mean().loc[min_ind[0]:min_ind[1]].mean()\n",
    "    return level_ice_elevation\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split ATM into multiple dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "elevation_threshold = .2\n",
    "length = 25000\n",
    "\n",
    "output_path = '/Users/torka/Library/CloudStorage/OneDrive-Personal/MarineSciences/MasterThs-T/Data/OIB/ATM/2014/ATM_levelled_classified'\n",
    "atm_path = '/Users/torka/Library/CloudStorage/OneDrive-Personal/MarineSciences/MasterThs-T/Data/OIB/ATM/2014'\n",
    "\n",
    "files = []\n",
    "for root, dirs, filenames in os.walk(atm_path):\n",
    "    for filename in filenames:\n",
    "        if filename.endswith('.h5'):\n",
    "            files.append(os.path.join(root, filename))\n",
    "\n",
    "files = np.sort(files)\n",
    "# print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "writing in smaller block size to make it easier to subset later in the snow radar matching. AND FOR LEVEL ICE SURFACE (ssh changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "325f9578a7364752bccf4692962f40a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "polys = []\n",
    "ks = []\n",
    "dates = []\n",
    "iss = []\n",
    "\n",
    "for k, file in enumerate(tqdm(files)):\n",
    "\n",
    "    date_str = file.split('_')[1]\n",
    "    f = h5py.File(os.path.join(atm_path, file), 'r')\n",
    "    elev = f['elevation']\n",
    "    lon = f['longitude']\n",
    "    lat = f['latitude']\n",
    "    time = f['instrument_parameters']['time_hhmmss']\n",
    "    \n",
    "    df_ATM = pd.DataFrame({'time':time, 'lon':lon, 'lat':lat, 'elev':elev})\n",
    "    df_ATM['x'], df_ATM['y'] = transformer.transform(df_ATM['lon'].values, df_ATM['lat'].values)\n",
    "\n",
    "    blocks = np.arange(0, len(df_ATM) + length, length)\n",
    "\n",
    "    for i, l in enumerate(blocks[:-1]):\n",
    "        subset =  df_ATM[l:blocks[i+1]].copy()\n",
    "        level_ice_elevation = get_level_ice_elevation(subset['elev'])\n",
    "        subset['elev_levelled'] = subset['elev'] - level_ice_elevation\n",
    "        subset['classes'] = subset['elev_levelled'].apply(lambda x: 1 if x > elevation_threshold else 0)\n",
    "        \n",
    "        # subset = subset.loc[subset['classes'] == 1] \n",
    "        filename = f'{date_str}_ATM_levelled_{k}_{i}.csv'\n",
    "        subset.to_csv(os.path.join(output_path, filename), columns=['time','x','y','elev_levelled','classes'], index=False)\n",
    "        \n",
    "        bbox = (subset['x'].min(), subset['y'].min(), subset['x'].max(), subset['y'].max())\n",
    "        \n",
    "        polys.append(bbox)\n",
    "        dates.append(date_str)\n",
    "        ks.append(k)\n",
    "        iss.append(i)\n",
    "\n",
    "df_poly = pd.DataFrame({'date':dates, 'bbox':polys, 'k':ks, 'i':iss})\n",
    "df_poly.to_csv(os.path.join(output_path, 'polygons.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 20160420, 5\n",
      "250000 to 300000...\n",
      "Block mean done...\n",
      "Grid done...\n",
      "Masking done...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 83\u001b[0m\n\u001b[1;32m     76\u001b[0m mask[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclasses\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m classes\n\u001b[1;32m     77\u001b[0m mask \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39massign_attrs(grid_spacing\u001b[38;5;241m=\u001b[39mspacing,\n\u001b[1;32m     78\u001b[0m                         level_ice\u001b[38;5;241m=\u001b[39mlevel_ice,\n\u001b[1;32m     79\u001b[0m                         classes_threshold\u001b[38;5;241m=\u001b[39melevation_threshold,\n\u001b[1;32m     80\u001b[0m                         bbox_xy \u001b[38;5;241m=\u001b[39m bbox_xy\n\u001b[1;32m     81\u001b[0m                         )\n\u001b[0;32m---> 83\u001b[0m \u001b[43mmask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_netcdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData written to file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Clear the output after each inner loop\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/landlab/lib/python3.10/site-packages/xarray/core/dataset.py:2346\u001b[0m, in \u001b[0;36mDataset.to_netcdf\u001b[0;34m(self, path, mode, format, group, engine, encoding, unlimited_dims, compute, invalid_netcdf, auto_complex)\u001b[0m\n\u001b[1;32m   2343\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   2344\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_netcdf\n\u001b[0;32m-> 2346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_netcdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[return-value]  # mypy cannot resolve the overloads:(\u001b[39;49;00m\n\u001b[1;32m   2347\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2350\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2354\u001b[0m \u001b[43m    \u001b[49m\u001b[43munlimited_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munlimited_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmultifile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2357\u001b[0m \u001b[43m    \u001b[49m\u001b[43minvalid_netcdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minvalid_netcdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauto_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauto_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2359\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/landlab/lib/python3.10/site-packages/xarray/backends/api.py:1888\u001b[0m, in \u001b[0;36mto_netcdf\u001b[0;34m(dataset, path_or_file, mode, format, group, engine, encoding, unlimited_dims, compute, multifile, invalid_netcdf, auto_complex)\u001b[0m\n\u001b[1;32m   1886\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1887\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m multifile \u001b[38;5;129;01mand\u001b[39;00m compute:  \u001b[38;5;66;03m# type: ignore[redundant-expr]\u001b[39;00m\n\u001b[0;32m-> 1888\u001b[0m         \u001b[43mstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m compute:\n\u001b[1;32m   1891\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/landlab/lib/python3.10/site-packages/xarray/backends/netCDF4_.py:597\u001b[0m, in \u001b[0;36mNetCDF4DataStore.close\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclose\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 597\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/landlab/lib/python3.10/site-packages/xarray/backends/file_manager.py:234\u001b[0m, in \u001b[0;36mCachingFileManager.close\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    232\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_key, default)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m     \u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for k, file in enumerate(files):\n",
    "\n",
    "    date_str = file.split('_')[1]\n",
    "    f = h5py.File(os.path.join(atm_path, file), 'r')\n",
    "    elev = f['elevation']\n",
    "    lon = f['longitude']\n",
    "    lat = f['latitude']\n",
    "    time = f['instrument_parameters']['time_hhmmss']\n",
    "    df_ATM = pd.DataFrame({'time':time, 'lon':lon, 'lat':lat, 'elev':elev})\n",
    "\n",
    "    df_ATM['x'], df_ATM['y'] = transformer.transform(df_ATM['lon'].values, df_ATM['lat'].values)\n",
    "\n",
    "    level_ice_elevation = get_level_ice_elevation(df_ATM['elev'])\n",
    "    df_ATM['elev_levelled'] = df_ATM['elev'] - level_ice_elevation\n",
    "    \n",
    "    blocks = np.arange(0, len(df_ATM) + length, length)\n",
    "    print(f'Length: {len(df_ATM)}, Blocks: {len(blocks)}')\n",
    "\n",
    "    for i, l in enumerate(blocks[:-1]):\n",
    "        \n",
    "        filename = f'{date_str}_ATM_gridded_{k}_{i}.nc'\n",
    "        \n",
    "        if filename in os.listdir(output_path) or date_str == '20160419':\n",
    "            if filename in os.listdir(output_path):\n",
    "                print(f'File already exists: {filename}')\n",
    "            if date_str == '20160419':\n",
    "                print(f'{date_str}, not considered...')\n",
    "            continue\n",
    "        \n",
    "        else:\n",
    "            print(f'Date: {date_str}, {i}')\n",
    "                \n",
    "            subset =  df_ATM[l:blocks[i+1]].copy()\n",
    "            print(f'{l} to {blocks[i+1]}...')\n",
    "            \n",
    "            # compute gridded elevations\n",
    "            blockmean = vd.BlockMean(spacing=spacing, center_coordinates=True)\n",
    "            block_coordinates, block_elev, block_weights = blockmean.filter(\n",
    "                coordinates=(subset['x'], subset['y']),\n",
    "                data=subset['elev_levelled'],\n",
    "            )\n",
    "            print(f'Block mean done...')\n",
    "            \n",
    "            cubic = vd.KNeighbors()\n",
    "            cubic.fit(\n",
    "                coordinates=(block_coordinates[0], block_coordinates[1]),\n",
    "                data=block_elev\n",
    "                )\n",
    "\n",
    "            grid = cubic.grid(spacing=spacing)\n",
    "            print(f'Grid done...')\n",
    "            \n",
    "            mask = vd.distance_mask(\n",
    "                (subset['x'], subset['y']),\n",
    "                maxdist=spacing * 3,\n",
    "                grid=grid\n",
    "            )\n",
    "            mask = mask.rio.write_crs(\"EPSG:3413\")\n",
    "            mask = mask.rename_vars({'scalars': 'elevation'})\n",
    "            print(f'Masking done...')\n",
    "            \n",
    "            # calculate dem attributes using xdem\n",
    "            slope = get_terrain_attribute(mask['elevation'].values, [\"slope\"], resolution=spacing, edge_method='nearest')\n",
    "            roughness = get_terrain_attribute(mask['elevation'].values, [\"roughness\"], resolution=spacing, edge_method='nearest')\n",
    "\n",
    "            mask['slope'] =  (('northing', 'easting'), slope)\n",
    "            mask['roughness'] =  (('northing', 'easting'), roughness)\n",
    "\n",
    "            # recompute level ice elevation, because of interpolation\n",
    "            # (should not be much different - ~cm differences)\n",
    "            level_ice = get_level_ice_elevation(mask['elevation'].values.flatten())\n",
    "            classes = xr.where(mask['elevation'] > level_ice + elevation_threshold , 1, 0 )\n",
    "            \n",
    "            bbox_xy = (mask['easting'].min().values, mask['northing'].min().values, mask['easting'].max().values, mask['northing'].max().values)\n",
    "            \n",
    "            mask['classes'] = classes\n",
    "            mask = mask.assign_attrs(grid_spacing=spacing,\n",
    "                                    level_ice=level_ice,\n",
    "                                    classes_threshold=elevation_threshold,\n",
    "                                    bbox_xy = bbox_xy\n",
    "                                    )\n",
    "\n",
    "            mask.to_netcdf(os.path.join(output_path, filename))\n",
    "            print(f'Data written to file: {filename}')\n",
    "            \n",
    "            # Clear the output after each inner loop\n",
    "            clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "landlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
