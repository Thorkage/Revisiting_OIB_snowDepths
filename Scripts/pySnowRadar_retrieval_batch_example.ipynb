{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch SnowRadar Processing Example\n",
    "A simple workflow using multiple CPUs and landmask/QA filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Community imports\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "import sys\n",
    "sys.path.append('/Users/torka/Library/CloudStorage/OneDrive-Personal/MarineSciences/MasterThs-T/pySnowRadar')\n",
    "\n",
    "\n",
    "# pySnowRadar imports|\n",
    "from pySnowRadar import SnowRadar\n",
    "from pySnowRadar.qc import error_check\n",
    "from pySnowRadar.processing import geo_filter,geo_filter_insitu_sites, batch_process\n",
    "from pySnowRadar.algorithms import Wavelet_TN, Peakiness\n",
    "from pySnowRadar.processing import extract_layers\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy.signal import find_peaks\n",
    "from datetime import datetime, timedelta\n",
    "from thefuzz import process, fuzz\n",
    "from scipy.spatial.distance import cdist\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BASIN-SCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sr_data_path = '/Volumes/PortableSSD/OIB_2016/no19th/*_deconv.nc'\n",
    "output_path = '/Volumes/PortableSSD/OIB_2016/Peakiness_v1'\n",
    "ATM_folder  = '/Volumes/PortableSSD/ATM_levelled_classified_v2'\n",
    "\n",
    "load_filtered = True\n",
    "log_name = 'log.csv'\n",
    "\n",
    "\n",
    "if load_filtered:\n",
    "        \n",
    "        insitu_site_filtered = pd.read_csv(os.path.join(output_path, log_name), index_col=0).values.flatten()\n",
    "else:\n",
    "        \n",
    "        input_sr_data = glob(input_sr_data_path)\n",
    "        insitu_site_filtered = geo_filter(input_sr_data)\n",
    "        pd.Series(insitu_site_filtered).to_csv(os.path.join(output_path, log_name))\n",
    "\n",
    "\n",
    "done_files = os.listdir(output_path)\n",
    "done_files = np.array([os.path.join('/Volumes/PortableSSD/OIB_2016/no19th/', x) for x in done_files if x.endswith('.nc') and x.startswith('I')])\n",
    "print(len(insitu_site_filtered))\n",
    "insitu_site_filtered = np.setdiff1d(insitu_site_filtered, done_files)\n",
    "print(len(insitu_site_filtered))\n",
    "\n",
    "        \n",
    "print('Geo-filtering done')\n",
    "\n",
    "\n",
    "# Generate error codes for SR data\n",
    "# sr_data = [SnowRadar(sr, 'full') for sr in insitu_site_filtered]\n",
    "# error_codes = [pd.Series(error_check(sr).tolist()) for sr in sr_data]\n",
    "\n",
    "workers = 8\n",
    "# picker = Wavelet_TN\n",
    "picker = Peakiness\n",
    "\n",
    "# FOR WAVELET_TN\n",
    "# params = {\n",
    "#         'snow_density': 0.3,\n",
    "#         'ref_snow_layer': 0.5,\n",
    "#         'cwt_precision': 10\n",
    "#         }\n",
    "\n",
    "params = {\n",
    "        'snow_density':0.3,\n",
    "        'log_peak_threshold' : 0.4,\n",
    "        'lin_peak_threshold' : 0.4, \n",
    "        'pp_r_threshold' : 30, \n",
    "        'pp_l_threshold' : 30\n",
    "        }\n",
    "\n",
    "res = batch_process(insitu_site_filtered, picker, params, workers,\n",
    "                dump_results = True,\n",
    "                overwrite = False,\n",
    "                path = output_path,\n",
    "                atm_folder = ATM_folder\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ECCC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_snow_density(path, sites):\n",
    "    files = os.listdir(path)\n",
    "    files = [f for f in files if f.endswith('csv')]\n",
    "    snow_densities_tmp = {}\n",
    "    for f in files:\n",
    "        # print(f)\n",
    "        site = f.split('_')[1]\n",
    "        \n",
    "        if site not in snow_densities_tmp.keys():\n",
    "            snow_densities_tmp[site] = pd.read_csv(os.path.join(path, f), index_col=0)\n",
    "        else:\n",
    "            snow_densities_tmp[site] = pd.concat([snow_densities_tmp[site], pd.read_csv(os.path.join(path, f), index_col=0)]) \n",
    "    snow_densities = {}\n",
    "    for site in sites:\n",
    "        snow_densities[site] = np.nanmean(snow_densities_tmp[site]['density'])/1000\n",
    "    return snow_densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WAVELET\n",
    "\n",
    "year = '2014'\n",
    "today = datetime.today().strftime('%Y%m%d')\n",
    "path_to_shapes = '/Users/torka/Library/CloudStorage/OneDrive-Personal/MarineSciences/MasterThs-T/Data/Eureka/grid_extents_v4/'\n",
    "\n",
    "if year == '2014':\n",
    "        campaign = 'EUREKA2014' \n",
    "        input_sr_data_path = '/Users/torka/Library/CloudStorage/OneDrive-Personal/MarineSciences/MasterThs-T/Data/OIB/Echograms/20140325/*/*_deconv.nc'        \n",
    "        output_path = f'/Users/torka/Library/CloudStorage/OneDrive-Personal/MarineSciences/MasterThs-T/Data/OIB/Wavelet/{today}_transect/'\n",
    "        # ATM_folder = ATM_folder ='/Users/torka/Library/CloudStorage/OneDrive-Personal/MarineSciences/MasterThs-T/Data/OIB/ATM/2014'\n",
    "        ATM_folder  = '/Users/torka/Library/CloudStorage/OneDrive-Personal/MarineSciences/MasterThs-T/Data/OIB/ATM/2014/ATM_levelled_classified'\n",
    "        sites = [2,3,4]\n",
    "        snow_densities = {2:0.3, 3:0.3, 4:0.3, 'transect':0.3}\n",
    "        \n",
    "elif year == '2016':    \n",
    "        campaign = 'EUREKA2016' \n",
    "        input_sr_data_path = '/Users/torka/Library/CloudStorage/OneDrive-Personal/MarineSciences/MasterThs-T/Data/OIB/Echograms/20160419/*/*_deconv.nc'        \n",
    "        output_path = f'/Users/torka/Library/CloudStorage/OneDrive-Personal/MarineSciences/MasterThs-T/Data/OIB/Wavelet/{today}_SRprecision/'\n",
    "        # ATM_folder = ATM_folder ='/Users/torka/Library/CloudStorage/OneDrive-Personal/MarineSciences/MasterThs-T/Data/OIB/ATM/2016'\n",
    "        snow_pit_path = '/Users/torka/Library/CloudStorage/OneDrive-Personal/MarineSciences/MasterThs-T/Data/Eureka/SnowPits/20250126'\n",
    "        sites = ['grid3', 'grid4', 'grid5', 'grid6', 'grid7', 'grid8'] \n",
    "        snow_densities = get_snow_density(snow_pit_path, sites)\n",
    "\n",
    "for site in tqdm(['transect']):\n",
    "# for site in tqdm(sites):\n",
    "\n",
    "        print(site)\n",
    "        input_sr_data = glob(input_sr_data_path)\n",
    "        # insitu_site_filtered = geo_filter(input_sr_data)\n",
    "        insitu_site_filtered = geo_filter_insitu_sites(path_to_shapes, year, site, input_sr_data)\n",
    "        print('Geo-filtering done')\n",
    "        \n",
    "        # Generate error codes for SR data\n",
    "        sr_data = [SnowRadar(sr, 'full') for sr in insitu_site_filtered]\n",
    "        print(len(sr_data))\n",
    "        \n",
    "        error_codes = [pd.Series(error_check(sr).tolist()) for sr in sr_data]\n",
    "\n",
    "        workers = 8\n",
    "        picker = Wavelet_TN\n",
    "\n",
    "        # FOR WAVELET_TN\n",
    "        params={\n",
    "                'snow_density': snow_densities[site],\n",
    "                'ref_snow_layer': 0.5,\n",
    "                'cwt_precision': 10\n",
    "                }\n",
    "\n",
    "        res = batch_process(insitu_site_filtered, picker, params, workers,\n",
    "                        dump_results = True,\n",
    "                        overwrite = True, \n",
    "                        path = os.path.join(output_path, str(site)),\n",
    "                        atm_folder = ATM_folder\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PEAKINESS\n",
    "picker = Peakiness\n",
    "\n",
    "year = '2014'\n",
    "today = datetime.today().strftime('%Y%m%d')\n",
    "path_to_shapes = '/Users/torka/Library/CloudStorage/OneDrive-Personal/MarineSciences/MasterThs-T/Data/Eureka/grid_extents_v4/'\n",
    "\n",
    "if year == '2014':\n",
    "        campaign = 'EUREKA2014' \n",
    "        input_sr_data_path = '/Users/torka/Library/CloudStorage/OneDrive-Personal/MarineSciences/MasterThs-T/Data/OIB/Echograms/20140325/*/*_deconv.nc'        \n",
    "        output_path = f'/Users/torka/Library/CloudStorage/OneDrive-Personal/MarineSciences/MasterThs-T/Data/OIB/Peakiness/{today}_transect/'\n",
    "        # ATM_folder = ATM_folder ='/Users/torka/Library/CloudStorage/OneDrive-Personal/MarineSciences/MasterThs-T/Data/OIB/ATM/2014'\n",
    "        ATM_folder  = '/Users/torka/Library/CloudStorage/OneDrive-Personal/MarineSciences/MasterThs-T/Data/OIB/ATM/2014/ATM_levelled_classified'\n",
    "        sites = [2,3,4]\n",
    "        snow_densities = {2:0.3, 3:0.3, 4:0.3, 'transect':0.3}\n",
    "        \n",
    "elif year == '2016':    \n",
    "        campaign = 'EUREKA2016' \n",
    "        input_sr_data_path = '/Users/torka/Library/CloudStorage/OneDrive-Personal/MarineSciences/MasterThs-T/Data/OIB/Echograms/20160419/*/*_deconv.nc'        \n",
    "        output_path = f'/Users/torka/Library/CloudStorage/OneDrive-Personal/MarineSciences/MasterThs-T/Data/OIB/Peakiness/{today}_SRprecision/'\n",
    "        # ATM_folder = ATM_folder ='/Users/torka/Library/CloudStorage/OneDrive-Personal/MarineSciences/MasterThs-T/Data/OIB/ATM/2016'\n",
    "        snow_pit_path = '/Users/torka/Library/CloudStorage/OneDrive-Personal/MarineSciences/MasterThs-T/Data/Eureka/SnowPits/20250126'\n",
    "        sites = ['grid3', 'grid4', 'grid5', 'grid6', 'grid7', 'grid8'] \n",
    "        snow_densities = get_snow_density(snow_pit_path, sites)\n",
    "\n",
    "for site in tqdm(['transect']):\n",
    "\n",
    "# for site in tqdm(sites):\n",
    "        print(site)\n",
    "        input_sr_data = glob(input_sr_data_path)\n",
    "        # insitu_site_filtered = geo_filter(input_sr_data)\n",
    "        insitu_site_filtered = geo_filter_insitu_sites(path_to_shapes, year, site, input_sr_data)\n",
    "        print('Geo-filtering done')\n",
    "        \n",
    "        # Generate error codes for SR data\n",
    "        sr_data = [SnowRadar(sr, 'full') for sr in insitu_site_filtered]\n",
    "        print(len(sr_data))\n",
    "        \n",
    "        error_codes = [pd.Series(error_check(sr).tolist()) for sr in sr_data]\n",
    "\n",
    "        workers = 8\n",
    "\n",
    "        # FOR PEAKINESS\n",
    "        params = {\n",
    "                'snow_density': snow_densities[site],\n",
    "                'log_peak_threshold' : 0.4,\n",
    "                'lin_peak_threshold' : 0.4, \n",
    "                'pp_r_threshold' : 30, \n",
    "                'pp_l_threshold' : 30\n",
    "                }\n",
    "\n",
    "        res = batch_process(insitu_site_filtered, picker, params, workers,\n",
    "                        dump_results=True,\n",
    "                        overwrite=True,\n",
    "                        path=os.path.join(output_path, str(site)),\n",
    "                        atm_folder=ATM_folder\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid3\n",
      "Geo-filtering done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:32<02:40, 32.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid4\n",
      "Geo-filtering done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [01:06<02:14, 33.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid5\n",
      "Geo-filtering done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [01:36<01:35, 31.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid6\n",
      "Geo-filtering done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [02:13<01:07, 33.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid7\n",
      "Geo-filtering done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [02:46<00:33, 33.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid8\n",
      "Geo-filtering done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [03:18<00:00, 33.02s/it]\n"
     ]
    }
   ],
   "source": [
    "#PEAKINESS\n",
    "year = '2016'\n",
    "today = datetime.today().strftime('%Y%m%d')\n",
    "path_to_shapes = '/Users/torka/Library/CloudStorage/OneDrive-Personal/MarineSciences/MasterThs-T/Data/Eureka/grid_extents_v3/'\n",
    "\n",
    "if year == '2014':\n",
    "        campaign = 'EUREKA2014' \n",
    "        input_sr_data_path = '/Users/torka/Library/CloudStorage/OneDrive-Personal/MarineSciences/MasterThs-T/Data/OIB/Echograms/20140325/*/*_deconv.nc'\n",
    "        output_path = f'/Users/torka/Library/CloudStorage/OneDrive-Personal/MarineSciences/MasterThs-T/Data/OIB/Peakiness/{today}/'\n",
    "        ATM_folder = ATM_folder ='/Users/torka/Library/CloudStorage/OneDrive-Personal/MarineSciences/MasterThs-T/Data/OIB/ATM/2014'\n",
    "        sites = [2,3,4]\n",
    "        \n",
    "elif year == '2016':    \n",
    "        campaign = 'EUREKA2016' \n",
    "        input_sr_data_path = '/Users/torka/Library/CloudStorage/OneDrive-Personal/MarineSciences/MasterThs-T/Data/OIB/Echograms/20160419/*/*_deconv.nc'\n",
    "        output_path = f'/Users/torka/Library/CloudStorage/OneDrive-Personal/MarineSciences/MasterThs-T/Data/OIB/Peakiness/{today}/'\n",
    "        ATM_folder = ATM_folder ='/Users/torka/Library/CloudStorage/OneDrive-Personal/MarineSciences/MasterThs-T/Data/OIB/ATM/2016'\n",
    "        \n",
    "        sites = ['grid3', 'grid4', 'grid5', 'grid6', 'grid7', 'grid8'] \n",
    "        \n",
    "for site in tqdm(sites):\n",
    "        print(site)\n",
    "        input_sr_data = glob(input_sr_data_path)\n",
    "        # insitu_site_filtered = geo_filter(input_sr_data)\n",
    "        insitu_site_filtered = geo_filter_insitu_sites(path_to_shapes, year, site, input_sr_data)\n",
    "        print('Geo-filtering done')\n",
    "        \n",
    "        # Generate error codes for SR data\n",
    "        sr_data = [SnowRadar(sr, 'full') for sr in insitu_site_filtered]\n",
    "        error_codes = [pd.Series(error_check(sr).tolist()) for sr in sr_data]\n",
    "\n",
    "        workers = 8    \n",
    "        picker = Peakiness\n",
    "\n",
    "        # FOR PEAKINESS\n",
    "        params = {\n",
    "                'snow_density':0.3,\n",
    "                'log_peak_threshold' : 0.4,\n",
    "                'lin_peak_threshold' : 0.4, \n",
    "                'pp_r_threshold' : 30, \n",
    "                'pp_l_threshold' : 30\n",
    "                }\n",
    "\n",
    "        res = batch_process(insitu_site_filtered, picker, params, workers,\n",
    "                        dump_results=True,\n",
    "                        overwrite=True,\n",
    "                        path = os.path.join(output_path, str(site)),\n",
    "                        atm_folder = None\n",
    "                        # atm_folder\n",
    "                        )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WAVELET SENSITIVITY ANALYSIS\n",
    "Setting different ref_snow_layer values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid3\n",
      "Geo-filtering done\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [03:57<19:46, 237.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid4\n",
      "Geo-filtering done\n",
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [08:40<17:37, 264.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid5\n",
      "Geo-filtering done\n",
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [10:35<09:47, 195.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid6\n",
      "Geo-filtering done\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [19:26<10:56, 328.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid7\n",
      "Geo-filtering done\n",
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [26:30<06:02, 362.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid8\n",
      "Geo-filtering done\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [30:26<00:00, 304.37s/it]\n"
     ]
    }
   ],
   "source": [
    "#WAVELET\n",
    "\n",
    "year = '2016'\n",
    "today = datetime.today().strftime('%Y%m%d')\n",
    "path_to_shapes = '/Users/torka/Library/CloudStorage/OneDrive-Personal/MarineSciences/MasterThs-T/Data/Eureka/grid_extents_v4/'\n",
    "\n",
    "if year == '2014':\n",
    "        campaign = 'EUREKA2014' \n",
    "        input_sr_data_path = '/Users/torka/Library/CloudStorage/OneDrive-Personal/MarineSciences/MasterThs-T/Data/OIB/Echograms/20140325/*/*_deconv.nc'        \n",
    "        output_path = f'/Volumes/PortableSSD/Wavelet_sensitivity'\n",
    "        ATM_folder = ATM_folder ='/Users/torka/Library/CloudStorage/OneDrive-Personal/MarineSciences/MasterThs-T/Data/OIB/ATM/2014'\n",
    "        sites = [2,3,4]\n",
    "        snow_densities = {2:0.3, 3:0.3, 4:0.3}\n",
    "        \n",
    "elif year == '2016':    \n",
    "        campaign = 'EUREKA2016' \n",
    "        input_sr_data_path = '/Users/torka/Library/CloudStorage/OneDrive-Personal/MarineSciences/MasterThs-T/Data/OIB/Echograms/20160419/*/*_deconv.nc'        \n",
    "        output_path = f'/Volumes/PortableSSD/Wavelet_sensitivity'\n",
    "        ATM_folder = ATM_folder ='/Users/torka/Library/CloudStorage/OneDrive-Personal/MarineSciences/MasterThs-T/Data/OIB/ATM/2016'\n",
    "        snow_pit_path = '/Users/torka/Library/CloudStorage/OneDrive-Personal/MarineSciences/MasterThs-T/Data/Eureka/SnowPits/20250126'\n",
    "        sites = ['grid3', 'grid4', 'grid5', 'grid6', 'grid7', 'grid8'] \n",
    "        snow_densities = get_snow_density(snow_pit_path, sites)\n",
    "\n",
    "\n",
    "for site in tqdm(sites):\n",
    "        print(site)\n",
    "        input_sr_data = glob(input_sr_data_path)\n",
    "        # insitu_site_filtered = geo_filter(input_sr_data)\n",
    "        insitu_site_filtered = geo_filter_insitu_sites(path_to_shapes, year, site, input_sr_data)\n",
    "        print('Geo-filtering done')\n",
    "        \n",
    "        # Generate error codes for SR data\n",
    "        sr_data = [SnowRadar(sr, 'full') for sr in insitu_site_filtered]\n",
    "        print(len(sr_data))\n",
    "        \n",
    "        error_codes = [pd.Series(error_check(sr).tolist()) for sr in sr_data]\n",
    "\n",
    "        workers = 4\n",
    "        picker = Wavelet_TN\n",
    "\n",
    "\n",
    "        for ref_snow_layer in [0.2, 0.3, 0.4, 0.5, 0.75, 1, 1.5]:\n",
    "\n",
    "            # FOR WAVELET_TN\n",
    "            params={\n",
    "                    'snow_density': snow_densities[site],\n",
    "                    'ref_snow_layer': ref_snow_layer,\n",
    "                    'cwt_precision': 10\n",
    "                    }\n",
    "\n",
    "            res = batch_process(insitu_site_filtered, picker, params, workers,\n",
    "                            dump_results=True,\n",
    "                            overwrite=True, \n",
    "                            path = os.path.join(output_path, str(site), str(ref_snow_layer)),\n",
    "                            atm_folder = None\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PEAKINESS SENSITIVITY ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting computation\n"
     ]
    }
   ],
   "source": [
    "outer_path = '/Users/torka/Library/CloudStorage/OneDrive-Personal/MarineSciences/MasterThs-T/Data/OIB/Peakiness_tuning'\n",
    "path_to_shapes = '/Users/torka/Library/CloudStorage/OneDrive-Personal/MarineSciences/MasterThs-T/Data/Eureka/grid_extents_v4/'\n",
    "year = '2016'\n",
    "\n",
    "file_list = []\n",
    "for root, dirs, files in os.walk('/Users/torka/Library/CloudStorage/OneDrive-Personal/MarineSciences/MasterThs-T/Data/OIB/Wavelet/20250112'):\n",
    "    for file in files:\n",
    "        file_list.append(os.path.join(root, file))\n",
    "sr_files = [x.split('/')[-1] for x in file_list if x.endswith('.nc')]\n",
    "\n",
    "input_sr_data = glob('/Users/torka/Library/CloudStorage/OneDrive-Personal/MarineSciences/MasterThs-T/Data/OIB/Echograms/20160419/*/*_deconv.nc')\n",
    "input_sr_data = [x for x in input_sr_data if x.split('/')[-1] in sr_files]\n",
    "\n",
    "\n",
    "sites = ['grid3', 'grid4', 'grid5', 'grid6', 'grid7', 'grid8']\n",
    "insitu_site_filtered = {}\n",
    "for site in sites:\n",
    "    insitu_site_filtered[site]= geo_filter_insitu_sites(path_to_shapes, year, site, input_sr_data)\n",
    "\n",
    "# Generate error codes for SR data\n",
    "# sr_data = [SnowRadar(sr, 'full') for sr in insitu_site_filtered]\n",
    "# error_codes = [pd.Series(error_check(sr).tolist()) for sr in sr_data]\n",
    "\n",
    "print('starting computation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for site in sites:\n",
    "    outer_path = '/Users/torka/Library/CloudStorage/OneDrive-Personal/MarineSciences/MasterThs-T/Data/OIB/Peakiness_tuning'\n",
    "    outer_path = os.path.join(outer_path, site)\n",
    "    Path(outer_path).mkdir(exist_ok=True)\n",
    "    \n",
    "    for log_peak_threshold in [0.4, 0.5, 0.6, 0.7, 0.8]:\n",
    "        for lin_peak_threshold in [0.2, 0.3, 0.4, 0.5]:\n",
    "            for pp_r_threshold in [20, 25, 30, 35]:\n",
    "                for pp_l_threshold in [20, 25, 30, 35]:\n",
    "                    \n",
    "                    folder_name = str(log_peak_threshold) + '_' + str(lin_peak_threshold) + '_' + str(pp_r_threshold) + '_' + str(pp_l_threshold)\n",
    "                    inner_path = os.path.join(outer_path, folder_name)\n",
    "                    Path(inner_path).mkdir(exist_ok=True)\n",
    "\n",
    "                    params={\n",
    "                        'snow_density': 0.3,\n",
    "                        'log_peak_threshold' : log_peak_threshold,\n",
    "                        'lin_peak_threshold' : lin_peak_threshold, \n",
    "                        'pp_r_threshold' : pp_r_threshold, \n",
    "                        'pp_l_threshold' : pp_l_threshold\n",
    "                    }\n",
    "\n",
    "                    workers = 8\n",
    "                    picker = Peakiness\n",
    "                    res = batch_process(insitu_site_filtered[site],\n",
    "                                        picker,\n",
    "                                        params,\n",
    "                                        workers,\n",
    "                                        dump_results=True,\n",
    "                                         overwrite=True,\n",
    "                                        path=inner_path,\n",
    "                                        atm_folder=None\n",
    "                                        )\n",
    "                    \n",
    "        print('log done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snowradar_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
